{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29e092b",
   "metadata": {},
   "source": [
    "### Linear regression model with a single predictor\n",
    "\n",
    "All the examples of linear regression models we have seen today share a common form: \n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n",
    "$$\n",
    "$$\n",
    "\\epsilon_i \\sim N(0, \\sigma)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\beta_0$ and $\\beta_1$ are some fixed numbers\n",
    "- $\\epsilon_i$ are independent, normally distributed variables with $E(\\epsilon_i)=0$ and $SD(\\epsilon_i)=\\sigma$, where $\\sigma$ is a strictly positive number.\n",
    "\n",
    "**Model parameters**: The models differ by assigning different values to the parameters\n",
    "- $\\beta_0, \\beta_1$: the regression coefficients\n",
    "- $\\sigma$: the standard deviation of the residua\n",
    "\n",
    "\n",
    "A statistical model with this form is called a **linear regression model** with a single predictor (or **univariate linear regression**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed657b3a",
   "metadata": {},
   "source": [
    "## Key assumptions of the linear regression model\n",
    "\n",
    "1) __Linearity:__ $Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\\ E(\\epsilon_i)=0$\n",
    "2) __Normally distributed residuals:__ The residuals $\\epsilon_i$ are normally distributed.\n",
    "3) __Homoscedasticity:__ The standard deviation of the residuals does not depend on the value of $X_i$.\n",
    "4) __Independence:__ The residuals $\\epsilon_i$ are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ols model\n",
    "est_mod = ols('y~bmi+sex+age', data=diab).fit()\n",
    "est_mod.params.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if need to **2/**3 \n",
    "est_mod = ols('pid7~urbancity+age+I(age**2)+I(age**3)', data=fc).fit()\n",
    "est_mod.params.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see p and 95%\n",
    "est_mod.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667aa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_estmod['exp_y'] = est_mod.predict() # this function computes E(Y) adding a new column in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64ca06",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a24aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logm = smf.logit('Survived~Fare',data=dft).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a logistic regression using categorical variables.\n",
    "modelp = smf.logit('Survived~C(Pclass)',data=dft).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4150aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interaction Terms in Logistic Regressions\n",
    "modelp = smf.logit('Survived~Sex+Fare+Sex*Fare',data=dft).fit()\n",
    "modelp.params\n",
    "\n",
    "## interpretation \n",
    "#The log-odd of survival will be 2.099345 lower when sex=male compared to female; \n",
    "#The log-odd of survival will be 0.019878 higher when Fareprice increase by 1   \n",
    "# the estimate for the interaction term means how much the effect of sex/fareprice on survival depends on each other. \n",
    "#in this case, the coefficient is nefative, so we can say for example the effect of fareprice on survival decreases when a person is a male.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e5532",
   "metadata": {},
   "source": [
    "#### Language for Interpretation\n",
    "\n",
    "$\\beta_1$ : The amount by which we expect the outcome $Y$ to increase for each 1-unit increase in $X_1$\n",
    "\n",
    "$\\beta_2$ : The amount by which we expect the outcome $Y$ to increase for each 1-unit increase in $X_2$\n",
    "\n",
    "95% confidence interval: The range within which we would expect the true value of a parameter. If multiple samples were drawn from the same population and a 95% CI calculated for each sample, we would expect the true parameter to be found within 95% of these CIs.\n",
    "\n",
    "Interaction coefficient: A statistically significant interaction terms asserts that the relationship between $X_1$ and $Y$ is conditional upon another variable $X_2$. In this case, the relationship / slope depends on the value of $X_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770dd7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ols model to Estimate the average treatment effect\n",
    "\n",
    "mod = ols(formula='y ~ treatment', data=df)\n",
    "res = mod.fit(cov_type = \"HC0\")\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc886d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the random assignment assumption by checking covariate balance on age(one covariate) (i.e. run a linear regression of age on treatment\n",
    "mod = ols(formula='age ~ treatment', data=df)\n",
    "res = mod.fit(cov_type = \"HC0\")\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use stargazer:\n",
    "from stargazer.stargazer import Stargazer\n",
    "\n",
    "mod = smf.ols(formula='age ~ treatment', data=df).fit(cov_type='HC0') \n",
    "Stargazer([mod])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670afbd",
   "metadata": {},
   "source": [
    "从dataframe中截取部分数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pretrend =  df[(df.time >= -8) & (df.time <= 12)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0644640",
   "metadata": {},
   "source": [
    "create dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65922c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = pd.get_dummies(df_pre,columns=[\"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07adf30a",
   "metadata": {},
   "source": [
    "the coefficient on treatment means the difference in age between the treatment and control group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26554df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('treatment')['age'].mean()[1]-df.groupby('treatment')['age'].mean()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fc2e3",
   "metadata": {},
   "source": [
    "The standard way of estimating an average treatment effect under conditional random assignment is to:  \n",
    "add the conditioning variable (in this case glasses) to a linear regression alongside the treatment indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf59f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ols(formula='y ~ treatemnt + glasses', data=df)\n",
    "res = mod.fit(cov_type = \"HC0\")\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the conditional random assignment (CIA) assumption by checking covariate balance conditional on glass wearing\n",
    "mod = smf.ols(formula='age ~ treatemnt + glasses', data=df)\n",
    "res = mod.fit(cov_type = \"HC0\")\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ef52b",
   "metadata": {},
   "source": [
    "random assignment of the treatment (seeing obama) seems to hold (the coefficient on obama is small and not statistically significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do covariate balance check on a list of variables:\n",
    "\n",
    "#Defining a list of predetermined variables\n",
    "covariates = ['age', 'children', 'city', 'income', 'yrs_sin_start']\n",
    "models = []\n",
    "for var in covariates:\n",
    "    mod = ols(formula=f'{var} ~ D ', data = df).fit(cov_type='HC0') \n",
    "    models.append(mod)\n",
    "    \n",
    "    \n",
    "table_result = Stargazer(models)\n",
    "\n",
    "#Adding the variable names to each model\n",
    "table_result.custom_columns(covariates, [1 for i in range(len(covariates))])\n",
    "table_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2SLS model:\n",
    "\n",
    "# Note the order of the inputs\n",
    "# 1. the outcome variable (Y)\n",
    "# 2. exog has to hold our constant and any controls that we might add (here no controls)\n",
    "# 3. endog will hold our treatment variable (D)\n",
    "# 4. intruments will hold  our instrument variable (Z)\n",
    "\n",
    "df['const'] = 1\n",
    "ivmod = IV2SLS(df['Y'], exog = df['const'] ,endog=df['D'],instruments=df['Z']).fit(cov_type='robust')\n",
    "print(ivmod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e2799",
   "metadata": {},
   "source": [
    "if need to add control variable in the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c201d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['const'] = 1\n",
    "ivmod = IV2SLS(df['Y'], exog =  df[['const', 'control_va']]  ,endog=df['D'],instruments=df['Z']).fit(cov_type='robust')\n",
    "print(ivmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#95% CI\n",
    "ivmod.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtain appropriately clustered standard errors\n",
    "## for each of the ATEs \n",
    "\n",
    "mod3 = smf.ols(formula='cases ~ lockdown*t2', data=df0_2).fit(cov_type='cluster', cov_kwds={'groups': df0_2['muni_name']})\n",
    "mod3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d5233",
   "metadata": {},
   "source": [
    "Creating the time dummies  \n",
    "Creating the interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 为了保证原来的time column不丢失，要在新的dataframe里面把time变成dummy，最后再加回原来的dataframe中\n",
    "\n",
    "# Creating the time dummies\n",
    "df_pretrend_dum = pd.get_dummies(df_pretrend, columns=['time']).drop(\"time_0\", axis = 1)\n",
    "\n",
    "# Creating the interactions\n",
    "cols = df_pretrend_dum.loc[:, 'time_1':'time_12'].columns\n",
    "df_pretrend_dum[list(map(lambda x: x+\"xtreat\", cols))] = df_pretrend_dum[cols].apply(lambda x: x*df_pretrend_dum['lockdown'])\n",
    "\n",
    "print(df_pretrend_dum.shape)\n",
    "df_pretrend_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19e3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 0 as the reference in the regression model(when including catagory variables, the first one is always set as the reference)\n",
    "mod = ols(formula='cases ~ lockdown*C(time, Treatment(reference=0))', data=df_pre)\n",
    "res = mod.fit(cov_type='cluster', cov_kwds={'groups': df_pre['muni_id']})\n",
    "res.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a103af",
   "metadata": {},
   "source": [
    "supervised learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270abbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression()\n",
    "#fitting the model\n",
    "x = df.drop('y',axis=1)\n",
    "y = df['y']\n",
    "model_skl = ols.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27461bb",
   "metadata": {},
   "source": [
    "splitting data into training/testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y',axis=1)\n",
    "\n",
    "y = df['y']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression()\n",
    "model = ols.fit(x_train, y_train)\n",
    "# estimate y using the model\n",
    "yhat_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76393eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculation of MSE\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# Compute using the predicted and actual values\n",
    "mean_squared_error(y_test, yhat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE\n",
    "mean_squared_error(y_test, yhat_test, sample_weight=None,squared=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a945662",
   "metadata": {},
   "source": [
    "### polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430b295",
   "metadata": {},
   "source": [
    "polynomial regression approach (of degree k = 6). Remember that a polynomial regression of degree k = 6 just means estimating a linear regression like the following using OLS:\n",
    "\n",
    "$Y_i =\\beta_0 +\\beta_1X_i + \\beta_2X_i^2 +\\beta_3X_i^3 +\\beta_4X_i^4 +\\beta_5X_i^5 +\\beta_6X_i^6 + ε_i$\n",
    "\n",
    "First we can repeat our supervised learning exercise using polynomial regression on the first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X=df.iloc[:,1:]\n",
    "y=df.y\n",
    "\n",
    "# Tell the function the number of polynomials that you with to obtain and for your x\n",
    "newx = PolynomialFeatures(6).fit_transform(X)\n",
    "\n",
    "#Column names for transparency\n",
    "cols = ['x_'+str(i) for i in range(2,7)]\n",
    "\n",
    "#array to df\n",
    "newdf=pd.DataFrame(newx, columns = [\"const\", \"x\"]+cols)\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split dataset\n",
    "\n",
    "#defining x as all the variables in the dataframe besides from y\n",
    "X = newdf.iloc[:, 1:]\n",
    "\n",
    "#Splitting into the initial training & test data\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c465b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the model\n",
    "model_2 = ols.fit(x_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d6149",
   "metadata": {},
   "source": [
    "### Supervised learning using k Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bebc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#save the knn function\n",
    "knn = KNeighborsRegressor(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bec040",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:].copy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, df['y'], test_size = 0.2, random_state = 123)\n",
    "\n",
    "#Fitting the model\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "#Predicting\n",
    "yhat_test = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsRegressor()\n",
    "params_knn = {'n_neighbors': [1,5,10,15,20,25,30,35,40,45,50,60,80,100,150,200,250,300,400,500]}\n",
    "gscv1 = GridSearchCV(knn, params_knn, cv = 5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "# reshape x:\n",
    "X = df.drop('speed',axis=1)\n",
    "y = df['speed']\n",
    "        \n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "#Fitting the model\n",
    "gscv1.fit(x_train, y_train)\n",
    "\n",
    "#Predicting\n",
    "yhat_test = gscv1.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739d7f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d0336",
   "metadata": {},
   "source": [
    "## 4. Supervised learning, step-by-step\n",
    "\n",
    "As discussed in the slides, a standard prediction/supervised learning analysis typically goes through the following steps:\n",
    "\n",
    "    \n",
    "\n",
    "1. Split the data into a training and a test part\n",
    "2. Pick hyperparameters, often using cross-validation\n",
    "3. Train the model on the training data\n",
    "4. Asses model performance on the test data\n",
    "5. (Re-estimate model on the full data)\n",
    "6. Compute predictions in out-of-sample data\n",
    "\n",
    "**Very briefly describe what each of the above steps entails and why they are necessary.**\n",
    "\n",
    "\n",
    "We need to be able to compare how our model does out-of-sample (on data that it has never seen). Else we risk training a model that performs great on our own data but does poorly on unseen data - which would be a bummer when the goal is to train a model that is good at predicting. Key-word: we do not want to overfit.\n",
    "\n",
    "\n",
    "We want to choose the best model and most models entail parameters that have to be set a priori. By hypertuning our model we can find the model specifications that best predict our target within our training sample. Cross-validation allows us to get a better feel of how our model will perform out of sample - our model is train on several train-test splits instead of one. By not saving the test set for last we will bias our model and once again face the risk of overfitting.\n",
    "\n",
    "Example of 5-fold cv process:\n",
    "![fold](https://www.pitcherlist.com/wp-content/uploads/1_NyvaFiG_jXcGgOaouumYJQ-768x241.jpeg)\n",
    "\n",
    "\n",
    "We train our model on the training data, because we have to establish how to best predict our target with the features that we have, before we can apply our model to unseen data.\n",
    "\n",
    "We asses model performance on test data, as we want to validate how our model(s) do(es) on out of sample data - check for overfitting etc.\n",
    "\n",
    "We re-estimate our model on the full data as we want to train our final model on as much availible information as possible. \n",
    "\n",
    "We compute out-of-sample predictions in order to fufill the purpose of having trained our model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf02c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2728b5",
   "metadata": {},
   "source": [
    "### hyperparameters using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfe1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### example:   find the best K\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Create the knn function\n",
    "knn = KNeighborsRegressor()\n",
    "params_knn = {'n_neighbors': [1,5,10,15,20,25,30,35,40,45,50,60,80,100,150]}\n",
    "gscv1 = GridSearchCV(knn, params_knn, cv = 5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# reshape x:\n",
    "X = df.iloc[:,1:].copy()\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, df['y'], test_size = 0.2, random_state = 123)\n",
    "\n",
    "#Fitting the model\n",
    "gscv1.fit(x_train, y_train)\n",
    "        \n",
    "print(f'KNN based on data from sublearnbig2 ({df.shape[0]} obs): Best model: {gscv1.best_params_}, with mean (negative) MSE: {gscv1.best_score_.round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771260f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85611c",
   "metadata": {},
   "source": [
    "## classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bc5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
